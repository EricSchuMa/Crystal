# Scalarization
Following the Reward Hypothesis by Rich & Sutton a scalar reward is adequate for all sequential decision-making tasks. [[Multi-Objective Sequential Decision Making#MOMDP|MOMDPs]] can always be converted into single-objective [[Sequential Decision Making#Markov Decision Process|MDPs]] with additive rewards via **scalarization of the reward vector** via a **scalarization function**.

## Scalarization Function
In Reinforcement Learning, a *scalarization function* $f$, is a function that maps the multi-objective value $V^\pi$ to a *scalar value*.
$$V_w^\pi(s)=f(V^\pi(s),w)$$



## [[Optimization]]
Formulating a single-objective problem such that the solutions to the single-objective problem are [[Optimization#Pareto Optimality|Pareto optimal]]. Formulate an MDP s.t. the additive returns for all $\pi$ and $s$ are equal to the scalarized reward $V_w^\pi(s)$.

### Weighted sum (Linear Scalarization)
For each $i=1,...,p, w_i>0$ is the weight of objective $i$.
$$min_{x\in \Omega} \sum_{i=1}^p w_i f_i(x) \quad (1)$$
- generates **one** [[Optimization#Pareto Optimality|Pareto optimal]] solution
- In the convex case, if $x^*$ is Pareto optimal, there exists a set of weights s.t. $x^*$ is the solution of (1)

### Monotonically Increasing Scalarization Function
- Monotonically increasing function of the rewards.
- Linear function might be unable to accurately represent a user's preferences.
- Policy increases for one or more of it's objectives without decreasing any of the other objectives, the scalarized value also increases
- Since the function is monotonically increasing Pareto dominant solutions will be preferred

### Example: 
Train service that minimizes
- $f_1$ travel time
- $f_2$ number of trains
- $f_3$ number of passengers

#### Definition of the weights
**Transform each objective into monetary costs**
- Travel time: value of time
- Number of trains: cost of running a train
- Number of passengers: estimate the revenue generated by the passengers

### Goal programming
For each $i=1,...,p$ the "ideal" objective function is defined by the modeler **! not really optimizing the objective function**

$$min_{x \in \Omega} ||F(x) - g||_l$$
### Lexicographic optimization
Sort the objectives according to their respective importance (from most important $i=1$ to least important $i=p$)
- First Problem $f_1=min_{x_\in \Omega} f_1(x)$ 
- $L'th$ problem $f_l=min_{x_\in \Omega} f_l(x)$ subject to $x \in \Omega$ $$f_i(x) = f_i^*, i=1,...,l-1$$
### Epsilon-lexicographic optimization
For each objective $f_i$ define a tolerance $i=1,...,p, \epsilon_i \geq 0$ 
- $L'th$ problem $f_l=min_{x_\in \Omega} f_l(x)$ subject to $x \in \Omega$ $$f_i(x) \leq f_i^* + e_i, i=1,...,l-1$$
### [[Optimization#Constrained Optimization|Constrained Optimization]] 
Select a reference objective and impose and upper bound on each other objective
$$min_{x \in \Omega} f_l(x)$$ subject to $$f_i(x) \leq \epsilon_i, i \neq l$$
If a solution exists, it is [[Optimization#Weak Pareto Optimality|weakly Pareto optimal]].