# Multi-Armed Bandits
Choose an action from a discrete action set, e.g., choosing from a set of restaurants to maximize happiness.

Regret: Difference in reward between optimal choice and actual choices taken.

Strategies:
- Explore only
- Exploit only
- $\epsilon$-greedy
- Zero Regret (regret goes to zero as $T \rightarrow \inf$ 


## Contextual Bandits
Contextual bandit incorporates some additional state or side-information, and so represents a compromise between stateless bandits and full-blown RL scenarios.